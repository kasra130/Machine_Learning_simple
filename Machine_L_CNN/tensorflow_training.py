# -*- coding: utf-8 -*-
"""TensorFlow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13APeYhKEZpJU-WaLZjouS6o0ASmAM4n8
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np   #god arrays
# %tensorflow_version 2.x   ##needed for tensorflow onlt in notebook
import tensorflow as tf;    
import pandas as pd;   #for data analysis ... allows manipulation of columns and rows and data in general
import matplotlib.pyplot as plt;    # graphing
from __future__ import absolute_import, division, print_function, unicode_literals
from IPython.display import clear_output   #3specific for colab to clear input 
from six.moves import urllib
import tensorflow.compat.v2.feature_column as fc   #regression algo 
import tensorflow_probability as tfp;
from tensorflow import keras
from tensorflow.keras import datasets, layers, models

"""Tensor : vector in higher dimension """

somelist = tf.Variable([['myass','uo'],['o','oo'],['00','77']], tf.string);

tf.rank(somelist)
somelist.shape

"""Types of tensors :
variable, const , placeholder , sparetensor 

tenosr session :
with tf.Session() as sess:
tensor.eval()   running part of the graph 
"""

t = tf.ones([5,5,5,5])
t0= tf.zeros ([5,5,5,5])
''' print (t0)
print(t) '''
t = tf.reshape(t, [125, -1])
''' print(t) '''

"""Core Algorithms
1. linear regression
2. classification
3. clustering
4. hidden makov modles


"""

x=[1,2,4,6,8]
y=[2,4,6,8,10]
plt.plot(x, y, 'ro')
plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))
plt.show()
slope=x[0]/y[0]
b=y[0]+(slope*-x[0])
lineeq= 'y='+str(slope)+'x+'+str(b)

print(b)
print(slope)
print(lineeq)

# Load dataset.
dftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv') # training data
dfeval = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv') # testing data

y_train = dftrain.pop('survived')
y_eval = dfeval.pop('survived')
print(dftrain.head()) #head first 5 enteries
print(dfeval.head())  #The survived info has been snipped and stored since our input needs to be seprataed from the output 

print(y_train)

print(dftrain.loc[0], y_train.loc[0])
print(dftrain['age'])

dftrain.describe()

dftrain.shape

dftrain.age.hist(bins=20)

dftrain.sex.value_counts().plot(kind='barh')

dftrain['class'].value_counts().plot(kind='barh')

pd.concat([dftrain, y_train], axis=1).groupby('sex').survived.mean().plot(kind='barh').set_xlabel('% survive')

"""So far, it is clear that the majority are between 28-31
there are more third class , then first class and second class==least

There are way more men and women. 
Women have bout 80%survival rate and men only 20%

Training
"""

print(dfeval.shape)  #used for testing 

print(dftrain.shape)   #used for training

!pip install -q sklearn

CATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck',
                       'embark_town', 'alone']   #sperating string from numbers
NUMERIC_COLUMNS = ['age', 'fare']

feature_columns = []   #what we need to feed to the linear modle 
for feature_name in CATEGORICAL_COLUMNS:
  vocabulary = dftrain[feature_name].unique()  # gets a list of all unique values from given feature column
  feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary)) #column ....puts eveyrthing in one... idk

for feature_name in NUMERIC_COLUMNS:
  feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))

print(feature_columns)

print(vocabulary)
print(dftrain['embark_town'].unique())
print(w)

"""How to train :
Feed it data. it must be loaded in batches .... 32 enteries at a time 

epochs: how many times the modle is going to see the same data.
feed it data once.---- sucks
feed the same data again but in different way/order/manner. 
the modle will look at the same data from different points of view..... each cycle is an epoche




Overfeeding : too much data... where it memeorieses instead of learning the pattern ... but once it gets the new data, it can not classify, idenitfy anything.. 

input function into a data object tf.data.Dataset




"""

def make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32): ##taking patameters of the dtaframe , with the labels and numbler of epokes, shuffle and how many in each batxh 
  def input_function():  # inner function, this will be returned
    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))  # create tf.data.Dataset object with data and its label... fictionary representation of the dtaframe and their labels (yvalues)
    if shuffle:       
      ds = ds.shuffle(1000)  # randomize order of data
    ds = ds.batch(batch_size).repeat(num_epochs)  # split dataset into batches of 32 and repeat process for number of epochs
    return ds  # return a batch of the dataset
  return input_function  # return a function object for use

train_input_fn = make_input_fn(dftrain, y_train)  # here we will call the input_function that was returned to us to get a dataset object we can feed to the model
eval_input_fn = make_input_fn(dfeval, y_eval, num_epochs=1, shuffle=False) #This for testing,,, no shuffle and 1 ecpoc

linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns)
# We create a linear estimtor by passing the feature columns we created earlier     
#so it just in a way makes a bestfir line ...  this is what we make all the time .....

linear_est.train(train_input_fn)  # train
result = linear_est.evaluate(eval_input_fn)  # get model metrics/stats by testing on tetsing data


clear_output()  # clears consoke output
print(result['accuracy'])  # the result variable is simply a dict of stats about our model
print(result)

res= list(linear_est.predict(eval_input_fn))
print(dfeval.loc[0])
print(res[0]['probabilities'][0])
print(y_eval.loc[0])
print("PERSON TWO.....")
print(dfeval.loc[2])
print(res[2]['probabilities'][0])
print(y_eval.loc[2])

def checkDEATH_CHANCE(n):
  print(dfeval.loc[n])
  print('chance of dying:')
  print(res[n]['probabilities'][0])
  if (y_eval.loc[n] == 0):
    vari='DIED'
  else :
      vari ='alive'

  print(y_eval.loc[n], vari)
  print("Person"+ str(n) +"chances")

for x in range (100):

  checkDEATH_CHANCE(x)

"""Training by classification 
iNSTEAD Of data being split into numeric value, each set is packed into a class and a more wholesome probability is given

###Dataset
This specific dataset seperates flowers into 3 different classes of species.
- Setosa
- Versicolor
- Virginica

The information about each flower is the following.
- sepal length
- sepal width
- petal length
- petal width
"""

CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']
SPECIES = ['Setosa', 'Versicolor', 'Virginica']
# Lets define some constants to help us later on

train_path = tf.keras.utils.get_file(
    "iris_training.csv", "https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv")   #keras is used (submodule of tensorfolow)  gettting this file and saving it as iris
test_path = tf.keras.utils.get_file(   #for taining 
    "iris_test.csv", "https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv")   #for testing 

train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)
test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)
# Here we use keras (a module inside of TensorFlow) to grab our datasets and read them into a pandas dataframe

train.head()   #speicies are defined numeatically   [ based on position in the list ]

train_y =train.pop('Species')
test_y = test.pop('Species')
train.head()

train.shape

train_y.head()

def input_fn(features, labels, training=True, batch_size=256):
    # Convert the inputs to a Dataset.
    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))

    # Shuffle and repeat if you are in training mode.
    if training:
        dataset = dataset.shuffle(1000).repeat()
    
    return dataset.batch(batch_size)

# Feature columns describe how to use the input.
my_feature_columns = []
for key in train.keys():
    my_feature_columns.append(tf.feature_column.numeric_column(key=key))
print(my_feature_columns)

"""Classification model : Two choices for this classification task 

DEEP neural netowrk :
This is a better choice ...


Linear Classifier : like the previous exmaple (linear regression) probability of ebing a lablel (rather than numerical value) 
"""

# Build a DNN with 2 hidden layers with 30 and 10 hidden nodes each.
classifier = tf.estimator.DNNClassifier(
    feature_columns=my_feature_columns,
    # Two hidden layers of 30 and 10 nodes respectively.
    hidden_units=[30, 10],
    # The model must choose between 3 classes.
    n_classes=3)

classifier.train(
    input_fn=lambda: input_fn(train, train_y, training=True), 
    steps=5000)      #steps are similar to epoch, just saying that stop when 5000 "things" have been looked at



 # '''  training the classifer :
 # The previous function was  def somefunction():
#                                def anotherfunc():
#Here however, we have one function ....   
#Lambda is a function and whatever is after the : is what a function does. (One liner function )

#e.g. x= lambda: print("hi") is prefectly valid  
#we are aagain calling a function of another function         '''
# We include a lambda to avoid creating an inner function previously

"""The lower the step loss ==> the better """

eval_result = classifier.evaluate(
    input_fn=lambda: input_fn(test, test_y, training=False))

print('\nTest set accuracy: {accuracy:0.3f}\n'.format(**eval_result))

"""Prediction """

def input_fn(features, batch_size=256):
    # Convert the inputs to a Dataset without labels.
    return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size)
 #
features = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth']
predict = {}

print("Please type numeric values as prompted.")
for feature in features:
  valid = True
  while valid: 
    val = input(feature + ": ")
    if not val.isdigit(): valid = False

  predict[feature] = [float(val)]

predictions = classifier.predict(input_fn=lambda: input_fn(predict))
for pred_dict in predictions:
    class_id = pred_dict['class_ids'][0]
    probability = pred_dict['probabilities'][class_id]

    print('Prediction is "{}" ({:.1f}%)'.format(
        SPECIES[class_id], 100 * probability))
    
    print(pred_dict)

"""Clustering 

Unsupervised learning algortihm .... VERY POWERFUL
hOWEVER it is specific . Clustering groups data points ... If you want to find a correlation bbetween data points and the data is essentually unlabled... cluster groups similar datas into a cathegory .

Basic Algorithm for K-Means.   this is the proceedure 
Step 1: Randomly pick K points to place K centroids
Step 2: Assign all the data points to the centroids by distance. The closest centroid to a point is the one it is assigned to.
Step 3: Average all the points belonging to each centroid to find the middle of those clusters (center of mass). Place the corresponding centroids into that position.
Step 4: Reassign every point once again to the closest centroid.
Step 5: Repeat steps 3-4 until no point changes which centroid it belongs to.
Please refer to the video for an explanation of KMeans clustering.

Caspers explanation :

Imagine randomly placed points on graph ....
pick 3 random regions ... evaluate the distance of each point to the centroid ..
The point belongs to the centroid closest to it (lets say centroid 1) .... and those points will all be packed in that specific centroid 1 which can be callled region 1 and it will be repeated intil we have 3 regions....  the data pakced into each region has some features / correlation / or something with each other ... (something in common) whihc allows them to be grouped wiht each other...
(E-G group one all have ginger hair, and are between 20-30 years old)



centroid then gets moved into the middle of its gorup (location is called center of mass)- (average of all the points)... 

after this  reassign the points to the closest centroid until none of the points are able to change their centroid -.
Now we have a cluster   . Now new data points can be properly assigned to a centroid .. 


IT IS important to lnow how many clusters are needed.

Markov Modles 


Porbability distribution . 
e.g. predicting weather given conditions . say if  sunny , then 80% chance of sunny again . and so on
Once the probabaility is dicovered it can b eused to predict future. 
We would have states => say two states ==> [Hot day], [cold day].These are hidden states \n because they are not directly observed, instead e.g. if 'Hot day' , then Casper has 80% chance of being happy. If 'Cold day' casper has 20% of being happpy. Casper being happy is an observation state , which is what we care about . 
 
###Data
Let's start by discussing the type of data we use when we work with a hidden markov model. 

In the previous sections we worked with large datasets of 100's of different entries. For a markov model we are only interested in probability distributions that have to do with states. 

We can find these probabilities from large datasets or may already have these values. We'll run through an example in a second that should clear some things up, but let's discuss the components of a markov model.

**States:** In each markov model we have a finite set of states. These states could be something like "warm" and "cold" or "high" and "low" or even "red", "green" and "blue". These states are "hidden" within the model, which means we do not direcly observe them.

**Observations:** Each state has a particular outcome or observation associated with it based on a probability distribution. An example of this is the following: *On a hot day Tim has a 80% chance of being happy and a 20% chance of being sad.*

**Transitions:** Each state will have a probability defining the likelyhood of transitioning to a different state. An example is the following: *a cold day has a 30% chance of being followed by a hot day and a 70% chance of being follwed by another cold day.*

To create a hidden markov model we need.
- States
- Observation Distribution
- Transition Distribution

For our purpose we will assume we already have this information available as we attempt to predict the weather on a given day.

In hot day 20% chance to cold day .
In cold day 30%  chance to hot day 
if hot day 80% next day is hot day. 
if cold day 70% chance next day is cold day .


Given mean, SD and max and min . . in hot day 
say average = 20 c
min= 15 
MAX= 25

SD would show the probability of change in temperature by some degrees (say SD 6c ) 
Given mean, SD and max and min . . in cold day 
say average = 5
min= -5
MAX= 15

The point is to predcit future based on the past events ...

###Weather Model
Taken direclty from the TensorFlow documentation (https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/HiddenMarkovModel). 

We will model a simple weather system and try to predict the temperature on each day given the following information.
1. Cold days are encoded by a 0 and hot days are encoded by a 1.
2. The first day in our sequence has an 80% chance of being cold.
3. A cold day has a 30% chance of being followed by a hot day.
4. A hot day has a 20% chance of being followed by a cold day.
5. On each day the temperature is
 normally distributed with mean and standard deviation 0 and 5 on
 a cold day and mean and standard deviation 15 and 10 on a hot day.

If you're unfamiliar with **standard deviation** it can be put simply as the range of expected values. 

In this example, on a hot day the average temperature is 15 and ranges from 5 to 25.

To model this in TensorFlow we will do the following.
"""

tfd = tfp.distributions  # making a shortcut for distribution module
initial_distribution = tfd.Categorical(probs=[0.8, 0.2])  # Refer to point 2 above : the initial probaility of being cold is 20% and then 80%
transition_distribution = tfd.Categorical(probs=[[0.7, 0.3],    # cold has 70% chance and then 30% chance of hot
                                                 [0.2, 0.8]])  # refer to points 3 and 4 above --- while hot has 20% to cold and 80% to hot 
observation_distribution = tfd.Normal(loc=[0., 15.], scale=[5., 10.])  # refer to point 5 above  standrad deviation on loc [meancold, Mean hot], [SD cold, SD hot]

# the loc argument represents the mean and the scale is the standard devitation

model = tfd.HiddenMarkovModel(
    initial_distribution=initial_distribution,
    transition_distribution=transition_distribution,
    observation_distribution=observation_distribution,
    num_steps=7)    #how to create the markov model

    #steps : How many days we want to predict.

mean = model.mean()     #probability calculation 

# due to the way TensorFlow works on a lower level we need to evaluate part of the graph
# from within a session to see the value of this tensor

# in the new version of tensorflow we need to use tf.compat.v1.Session() rather than just tf.Session()


with tf.compat.v1.Session() as sess:      #running the session 
  print(mean.numpy())

"""The output above is [The expected tempreture of the days (7 days). This is not traning... just probability ..

Neural Networks
 Point of it is to make classification or prediction  .... Input given to blackbox and useful output is taken. Maps an inout to an output , like a function. 

NN is made up of Layers .
First layer is the input layer (raw data). 
E.G. an image is shown, with widthxheight of 28x28. How many layers do you need?
we will need every single pixle (28x28=784) neurans -- each fking pixel is analysed by a nueran. 
Each piece of information is going to an inout neuran. 


Then we have the outout layer. The value of the output is between [1,0] if value is closer to 1 , then class 1 otherwise class 0. 
say 5 classes defined . then 5 output neruons needed , each [0,1], their sum euqals 1. its like a probablity distribution, where we compare the liklyhood of each output agaist the other and the one with the highest probaility wins and is the most likly the correct answer. 



Inbetween the layers (hideen layers). 
This layer is not observed , interacted with and frakly not many know whats happening in between these lauers. 
These layers work with weights, and are interconneected with each other. each weight has a value. 
The value of a nuron is determined by suming the value of the previous weights. 
Neuron = Wight of (nerons x,y and z)*thir value + weight of Bias. As the network gets better, the values of the weights change. 
Bias: There is only one bias and it exists in the previous layer that it effects. Bias takes no input information. It is a constant numeric value. The connection from the bias to another neuron alwaus the the weight of value 1. 

Densily conncted layers : layer where everything node is connected to the hidden layer. e.g. how many connections in the first hidden layer(2 nodes)==> say we have 3 input nodes- then they are all connected to the nodes of the first hidden layer 3x2 = 6 each node in the hidden layer having 3 connections from the input nodes. 

Activation function: There are few types 
1. Rectified Linear Unit (Relu) : Take any value less than 0 (negative values) and make them 0  . Point is to eliminate any negative numbers 

2. Hyperbolic Tangent (Tan(h)) : Takes value and rounds them to either -1 or 1 depending on how close the computed value is to -1 or 1. so -0.6 == -1 while 0.5 == 1


3. Sigmoig : Takes the value in between 0 and 1 and evaluates which it is closer to and round up to that value.  0.2 == 0 , wile 0.6 == 1. 

At each neuron there is an activation function which is applied to the weighted sum (value of the neuron) before it is sent to the next neuron. N1 = F(activation function (sum wi(xi) + b)).-... 
At the output neuron the activation is key.... do you want the value between 0-1?
between -1 and 1 ? or massive numbers? 0- inf and much more... 

If 0-1 is desired, then Sigmoid activation function is used to squeesh the value to either 0 or 1  . 

sig[N1w0 + N2W2 + b] = some value between 0 and 1. 
 The sigmoid function essentually takes in values of n dimension and spits out an scalar. (single value). Activation functions are used in Neural Networks because they introduce non-linearity which you need because otherwise your Neural Network would just be a linear classifier so it would basically do one big weighted summation of the inputs. This is not enough to capture complicated relationships in data. With non-linearity introduced, it can be shown that a Neural Network can approximate any function with arbitrary accuracy, that is any function that might have generated the data. Read up on the universal approximation theorem, if you want to learn more.

How to train  : In peanuts terms: Listen to what the neuron thinks the answer is ... show it the expected output and it should adjust itself and modify the weights to figure our the logic in your problem. 

Say asnwer == red and value = 0   , while blue == 1 
If the answer is 0.7, it is pretty far.. and here comes the loss fucntion 


Loss function : It calcculates how far away the output is from the expected output... 
The loss function gives the value (high loss means bad prediction). Based in the value probided by the loss function, the netwrok adjusts it self. 

Mean squared error : average the square of the error (better amplification) shows the error more 
Mean absolute error: average of the value of the error  (Pure error simple)
Hinge loss:  Peak (max) value 


How weights and bias values are updated: 
When the network is transformed into higher dimensions, it allows for more details and small little bits of information which increases the accuracy and equality but the complexity at he same time.
A square in 2d has wxh components with a premeter and area and thats all . 
Transform the square into 3D as a cube and wxhxl is introduced making complicated concepts such as volume, SA, and more complex phemomiums and more details. 
Gradient Descent : Optimization of the loss function . (get it as low as possible (global min))

Optimizer: Twiking .

OFFICIAL TRAINING 

Fashion dataset MNIST   , built into keras
"""

fashion_mnist = keras.datasets.fashion_mnist  # load dataset

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()  # split into tetsing and training

print(train_images.shape)
print(type(train_images))

train_images[0,23,23]  # one pixel  it is represnted bteween value of 0 to 255

train_labels[:10]   #10 first training labels represnted by numbers between 0-9

class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']    #telling what string each number refers to

plt.figure()
plt.imshow(train_images[1])
plt.colorbar()
plt.grid(False)
plt.show()      # gray scale

plt.figure()
plt.imshow(train_images[3])
plt.colorbar()
plt.grid(False)
plt.show()

plt.figure()
plt.imshow(train_images[8])
plt.colorbar()
plt.grid(False)
plt.show()

plt.figure()
plt.imshow(train_images[8])
plt.colorbar()
plt.grid(False)
plt.show()

"""##Data Preprocessing
The last step before creating our model is to *preprocess* our data. This simply means applying some prior transformations to our data before feeding it the model. In this case we will simply scale all our greyscale pixel values (0-255) to be between 0 and 1. We can do this by dividing each value in the training and testing sets by 255.0. We do this because smaller values will make it easier for the model to process our values.

Architecture of th netwrok
"""

train_images = train_images / 255.0

test_images = test_images / 255.0     ##to scale dwon the number to make computation easier before feeding to the network.

model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),  # input layer (1) flatten allows shape of 28x28 by flatting everything into 720 pixeles 
    keras.layers.Dense(128, activation='relu'),  # hidden layer (2)  Dense: all of the neurons in the previous layer are connected to the neurons on the next layer. WITH relu actovation function
    keras.layers.Dense(10, activation='softmax') # output layer (3) SAME shit with 10 output neurons ====> 10 classes...... 10 possibilirs 10 predictions needed and the softmax activation function makes sure 
    # that all of th values are between 0 and 1 
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])    # compiling the modle : adam is an optimization alogrithm ... , loss functio sparse, and the metrics is accuracy which is thea fraction represnting the correctness of the network

model.fit(train_images, train_labels, epochs=1)  # we pass the data, labels and epochs and watch the magic!  fit the modle data with 10 epochs

"""Now trained, lets test it .. """

test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)   #verbose (how much info?)

print('Test accuracy:', test_acc)

"""Accuracy is lower due to over feeding .... it saw that data so often ...
it is not good at generalizing  

"""

predictions = model.predict(test_images)
print(predictions)

print(class_names[np.argmax(predictions[6])])
plt.figure()
plt.imshow(test_images[6])
plt.colorbar()
plt.grid(False)
plt.show()

predictions[0]

np.argmax(predictions[0])

test_labels[0]

COLOR = 'white'
plt.rcParams['text.color'] = COLOR
plt.rcParams['axes.labelcolor'] = COLOR

def predict(model, image, correct_label):
  class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
  prediction = model.predict(np.array([image]))
  predicted_class = class_names[np.argmax(prediction)]

  show_image(image, class_names[correct_label], predicted_class)

  
def show_image(img, label, guess):
  plt.figure()
  plt.imshow(img, cmap=plt.cm.binary)
  print('expected to say ' + label);
  plt.title('Excpected: ' + label)
  plt.xlabel("Guess: " + guess)
  print('it guessed ' + guess);
  plt.colorbar()
  plt.grid(False)
  plt.show()


def get_number():
  while True:
    num = input("Pick a number: ")
    if num.isdigit():
      num = int(num)
      if 0 <= num <= 1000:
        return int(num)
    else:
      print("Try again...")

num = get_number()
image = test_images[num]
label = test_labels[num]
predict(model, image, label)

"""##Borders and Padding
The more mathematical of you may have realized that if we slide a filter of let's say size 3x3 over our image well consider less positions for our filter than pixels in our input. Look at the example below. 

*Image from "Deep Learning with Python" by Francois Chollet (pg 126).*
![alt text](https://drive.google.com/uc?export=view&id=1OEfXrV16NBjwAafgBfYYcWOyBCHqaZ5M)

This means our response map will have a slightly smaller width and height than our original image. This is fine but sometimes we want our response map to have the same dimensions. We can accomplish this by using something called *padding*.

**Padding** is simply the addition of the appropriate number of rows and/or columns to your input data such that each pixel can be centered by the filter.

##Strides
In the previous sections we assumed that the filters would be slid continously through the image such that it covered every possible position. This is common but sometimes we introduce the idea of a **stride** to our convolutional layer. The stride size reprsents how many rows/cols we will move the filter each time. These are not used very frequently so we'll move on.

##Pooling
You may recall that our convnets are made up of a stack of convolution and pooling layers.

The idea behind a pooling layer is to downsample our feature maps and reduce their dimensions. They work in a similar way to convolutional layers where they extract windows from the feature map and return a response map of the max, min or average values of each channel. Pooling is usually done using windows of size 2x2 and a stride of 2. This will reduce the size of the feature map by a factor of two and return a response map that is 2x smaller.dfdd

Padding is used inorder to get the algo to focus on specific section and ingore the rest of the image,,,

So when this is usually to egt the attention towards center since the edges normally tend to have less features and important details . 

Pooling is taking spcific values from the output feature map (the values computed from the comparision ) Take the min, max, average to a new feature map which is 2x smaller than the original map size.. 

POOL min: Does the feature not exist==?
Max Pool: is that feature in that area ?
Average Pooling: average presence of that feature in that area..
"""

#  LOAD AND SPLIT DATASET
(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()    #wierd dataset object

# Normalize pixel values to be between 0 and 1
train_images, test_images = train_images / 255.0, test_images / 255.0    #deviding to make compoutation easier 

class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']

# Let's look at a one image
IMG_INDEX = 6  # change this to look at other images

plt.imshow(train_images[IMG_INDEX] ,cmap=plt.cm.binary)
plt.xlabel(class_names[train_labels[IMG_INDEX][0]])
plt.show()

model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))    #(amount of filter, (how much the flter are) ) =>activation fucntion =relu and inputshapre(first layer 32x32x3)
model.add(layers.MaxPooling2D((2, 2)))    #Two by two smaple size with stride of 2 
model.add(layers.Conv2D(64, (3, 3), activation='relu'))   
model.add(layers.MaxPooling2D((2, 2)))      
model.add(layers.Conv2D(64, (3, 3), activation='relu'))    #presence of features 
model.summary()  # let's have a look at our model so far

model.add(layers.Flatten())   #flatten andmkae it all one dimentional
model.add(layers.Dense(64, activation='relu'))   #64 dense nuron layers interconnected
model.add(layers.Dense(10))      #output

model.summary()

model.compile(optimizer='adam',      
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']) #opitmiser is adam ... 

history = model.fit(train_images, train_labels, epochs=10, 
                    validation_data=(test_images, test_labels))  #training

test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)
print(test_acc)   #evaluation : The patterns are alot ,

from keras.preprocessing import image
from keras.preprocessing.image import ImageDataGenerator    #imagedatagenerator , allows aurgementation of image while training 
#It is very dfficult to create a good convo network with small data... 
#we have to use pretrained modles to get a good one. 
# creates a data generator object that transforms images
datagen = ImageDataGenerator(
rotation_range=40,
width_shift_range=0.2,
height_shift_range=0.2,
shear_range=0.2,
zoom_range=0.2,
horizontal_flip=True,
fill_mode='nearest')

# pick an image to transform
test_img = train_images[20]
img = image.img_to_array(test_img)  # convert image to numpy arry
img = img.reshape((1,) + img.shape)  # reshape image

i = 0

for batch in datagen.flow(img, save_prefix='test', save_format='jpeg'):  # this loops runs forever until we break, saving images to current directory with specified prefix
    plt.figure(i)
    plot = plt.imshow(image.img_to_array(batch[0]))
    i += 1
    if i > 4:  # show 4 images
        break

plt.show()

"""Pretrained models..
Using part of the trained convolution modle... with tweaking the last layers to suit our purpose. 
E.g the modle is amazing at generalzing , but we modify to only recognize for example cats and dogs . 
"""

import tensorflow_datasets as tfds
tfds.disable_progress_bar()

# split the data manually into 80% training, 10% testing, 10% validation
(raw_train, raw_validation, raw_test), metadata = tfds.load(
    'cats_vs_dogs',
    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],
    with_info=True,
    as_supervised=True,
)

get_label_name = metadata.features['label'].int2str  # creates a function object that we can use to get labels

# display 2 images from the dataset
for image, label in raw_train.take(5):
  plt.figure()
  plt.imshow(image)
  plt.title(get_label_name(label))

IMG_SIZE = 160 # All images will be resized to 160x160   its better to compress 

def format_example(image, label):
  """
  returns an image that is reshaped to IMG_SIZE
  """
  image = tf.cast(image, tf.float32)
  image = (image/127.5) - 1
  image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))
  return image, label

train = raw_train.map(format_example)
validation = raw_validation.map(format_example)
test = raw_test.map(format_example)

for image, label in train.take(2):
  plt.figure()
  plt.imshow(image)
  plt.title(get_label_name(label))

BATCH_SIZE = 32
SHUFFLE_BUFFER_SIZE = 1000

train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)
validation_batches = validation.batch(BATCH_SIZE)
test_batches = test.batch(BATCH_SIZE)

for img, label in raw_train.take(2):
  print("Original shape:", img.shape)

for img, label in train.take(2):
  print("New shape:", img.shape)

IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)

# Create the base model from the pre-trained model MobileNet V2
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')   #just training model   the input shape can be changed...
                                               #Include top: means do we include the classfier ? meaning 1000 different classes which the model was trained for.

for image, _ in train_batches.take(1):
   pass

feature_batch = base_model(image)
print(feature_batch.shape)  #the tensor gotten

base_model.trainable = False   #we want to use this as the base,,, not chnaged.. if we train, we chnage this... do not touch

base_model.summary()

global_average_layer = tf.keras.layers.GlobalAveragePooling2D() ## take average of 1280 different layers and aqverage them out into a 1d flattened tensor

prediction_layer = keras.layers.Dense(1)   #One dense node... make it thick

model = tf.keras.Sequential([
  base_model,
  global_average_layer,
  prediction_layer
])   # out 1 neruron from sense layer 
#1280 weights connected to 1 neuron ,,,,,, with 1 Biased

model.summary()

base_learning_rate = 0.0001
model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy']) #How mcuh am I allowed to modify the weights  (VERY LITTLE coz its already rly good )

# We can evaluate the model right now to see how it does before training it on our new images
initial_epochs = 3
validation_steps=20

loss0,accuracy0 = model.evaluate(validation_batches, steps = validation_steps)

# Now we can train it on our images
history = model.fit(train_batches,
                    epochs=initial_epochs,
                    validation_data=validation_batches)

acc = history.history['accuracy']
print(acc)   #we train this

model.save("dogs_vs_cats.h5")  # we can save the model and reload it at anytime in the future
new_model = tf.keras.models.load_model('dogs_vs_cats.h5')

